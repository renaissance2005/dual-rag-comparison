{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare all libraries this program will be using\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain_community.utilities.redis import get_client\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from IPython.display import HTML, display, Markdown\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import re\n",
    "import uuid\n",
    "import base64\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: poweredge-r740-spec-sheet.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# This section ingest the PDF files from the 'datasheet' folder\n",
    "# Load and process PDFs\n",
    "path = \"./datasheet/\"\n",
    "dir = os.listdir(path)\n",
    "docs = []\n",
    "\n",
    "for file in dir:\n",
    "    if file.endswith('.pdf'):\n",
    "        print(f\"Processing: {file}\")\n",
    "        docs.append(file)\n",
    "\n",
    "def process_pdf(doc):\n",
    "    try:\n",
    "        loader = UnstructuredPDFLoader(file_path=doc,\n",
    "                                    strategy='hi_res',\n",
    "                                    extract_images_in_pdf=True,\n",
    "                                    infer_table_structure=True,\n",
    "                                    chunking_strategy=\"by_title\",\n",
    "                                    max_characters=4000,\n",
    "                                    new_after_n_chars=4000,\n",
    "                                    combine_text_under_n_chars=2000,\n",
    "                                    mode='elements',\n",
    "                                    image_output_dir_path='./figures')\n",
    "        data = loader.load()\n",
    "        for item in data:\n",
    "            item.metadata['document_name'] = os.path.basename(doc)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {doc}: {e}\")\n",
    "        return []\n",
    "\n",
    "all_data = []\n",
    "for doc in docs:\n",
    "    pathdoc = path + doc\n",
    "    data = process_pdf(pathdoc)\n",
    "    all_data.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "chatgpt = Ollama(model=\"llama3.1:8b\")\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for textual data and tables\n",
    "\n",
    "docs = []\n",
    "tables = []\n",
    "\n",
    "for doc in data:\n",
    "    if doc.metadata['category'] == 'Table':\n",
    "        tables.append(doc)\n",
    "    elif doc.metadata['category'] == 'CompositeElement':\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate summaries for textual data and tables and store them in their respective lists\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text particularly for semantic retrieval.\n",
    "These summaries will be embedded and used to retrieve the raw text or table elements\n",
    "Give a detailed summary of the table or text below that is well optimized for retrieval.\n",
    "For any tables also add in a one line description of what the table is about besides the summary.\n",
    "Do not add additional words like Summary: etc.\n",
    "\n",
    "Table or text chunk:\n",
    "{element}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "summarize_chain = (\n",
    "                    {\"element\": RunnablePassthrough()}\n",
    "                      |\n",
    "                    prompt\n",
    "                      |\n",
    "                    chatgpt\n",
    "                      |\n",
    "                    StrOutputParser() # extracts the response as text and returns it as a string\n",
    ")\n",
    "\n",
    "# Initialize empty summaries\n",
    "text_summaries = []\n",
    "table_summaries = []\n",
    "\n",
    "text_docs = [doc.page_content for doc in docs]\n",
    "table_docs = [table.page_content for table in tables]\n",
    "\n",
    "text_summaries = summarize_chain.batch(text_docs, {\"max_concurrency\": 5})\n",
    "table_summaries = summarize_chain.batch(table_docs, {\"max_concurrency\": 5})\n",
    "\n",
    "len(text_summaries), len(table_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 13.602229833602905 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# various functions to process image\n",
    "\n",
    "# encode the image in base64\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# summarize the image based on base64 constitution and prompt given\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "   \n",
    "    chat = ChatOllama(model=\"llava-llama3\", temperature=0)\n",
    "    \n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "# generate image summaries and return 2 lists: base64 and summaries\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval.\n",
    "                Remember these images could potentially contain graphs, charts or tables also.\n",
    "                These summaries will be embedded and used to retrieve the raw image for question answering.\n",
    "                Give a detailed summary of the image that is well optimized for retrieval.\n",
    "                Do not add additional words like Summary: etc.\n",
    "             \"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Execute the function\n",
    "IMG_PATH = './figures'\n",
    "start_time = time.time()\n",
    "imgs_base64, image_summaries = generate_img_summaries(IMG_PATH)\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DAVID\\AppData\\Local\\Temp\\ipykernel_77156\\1730926640.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  chroma_db = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# initialize an instance of vector store\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"private_rag\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./privatedb\",\n",
    "    # collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the document store - to store raw images, text and tables\n",
    "client = get_client('redis://localhost:6379')\n",
    "redis_store = RedisStore(client=client) # you can use filestore, memorystory, any other DB store also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add summaries to vector store and raw contents to document store\n",
    "\n",
    "import uuid\n",
    "\n",
    "\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    docstore, vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=docstore,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    ## THE FOLLOWING SUPPOSED CAN BE COMMENTED OUT AND LEFT return retriever only if I have persisted vectorstore and docstore previously\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector = create_multi_vector_retriever(\n",
    "    redis_store,\n",
    "    chroma_db,\n",
    "    text_summaries,\n",
    "    text_docs,\n",
    "    table_summaries,\n",
    "    table_docs,\n",
    "    image_summaries,\n",
    "    imgs_base64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 image manipulation functions\n",
    "\n",
    "# This function only used for display image for debugging purpose\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Decode the base64 string\n",
    "    img_data = base64.b64decode(img_base64)\n",
    "    # Create a BytesIO object\n",
    "    img_buffer = BytesIO(img_data)\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(img_buffer)\n",
    "    display(img)\n",
    "\n",
    "# check if the string is base64\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "# check if the base64 string belong to an image\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# split base64-encoded images and texts\n",
    "def split_image_text_types(docs):\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content.decode('utf-8')\n",
    "        else:\n",
    "            doc = doc.decode('utf-8')\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section fo the code is used if the vectorstore and documentstore were populated previously. \n",
    "# All the code prior to this section do not have to be executed if all the documents were loaded previously. \n",
    "# Hence, the libraries are declared here assuming the first cell is not run\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain_community.embeddings import OllamaEmbeddings \n",
    "from langchain_community.storage import RedisStore # document store\n",
    "from langchain_community.utilities.redis import get_client\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"private_rag\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "client = get_client('redis://localhost:6379')\n",
    "docstore = RedisStore(client=client) # you can use filestore, memorystory, any other DB store also\n",
    "\n",
    "# Create the multi-vector retriever\n",
    "retriever_multi_vector = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "def multimodal_rag_qa(query):\n",
    "    response = multimodal_rag_w_sources.invoke({'input': query})\n",
    "    return response['answer'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The libraries are called again as there is a possibility that the first cell is not run\n",
    "# as in the case where internal documents (PDF) are not required to load again\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# This function accepts the two-dimensional array and generate inference from prompt constructed\n",
    "def multimodal_prompt_function(data_dict):\n",
    "    \"\"\"\n",
    "    Create a multimodal prompt with both text and image context.\n",
    "\n",
    "    This function formats the provided context from `data_dict`, which contains\n",
    "    text, tables, and base64-encoded images. It joins the text (with table) portions\n",
    "    and prepares the image(s) in a base64-encoded format to be included in a message.\n",
    "\n",
    "    The formatted text and images (context) along with the user question are used to\n",
    "    construct a prompt for multimodal model.\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = \"\"\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = f\"Image: data:image/jpeg;base64,{image}\\n\"\n",
    "            messages += image_message\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = f\"\"\"\n",
    "        You are an pre-sales specialist well-versed with the company products based on the datasheet and brochure available and\n",
    "        are competent in matching the requirements from potential user or clients with the available product specification. \n",
    "        You will be given Context documents which will be a mix of text, tables, and images usually of charts or graphs.\n",
    "        Use this information to match the specification for provided Minimum specification with the relevant specification offered in Context documents.\n",
    "        Your answer should specify a value or text extracted as a piece of information from Context document only. \n",
    "        DO NOT include preamble. \n",
    "        DO NOT give me the sources where you obtain your information or conclusion. \n",
    "        DO NOT ask further questions.\n",
    "        DO NOT make up answers, use the provided context documents below and answer the question to the best of your ability. \n",
    "        Be as brief, complete and precise in your output as possible. \n",
    "\n",
    "        #Example: \n",
    "        Minimum specification: 'Weight: Maximum 3kg'\n",
    "        Output: 'Minimum weight starts at 1.21kg'\n",
    "        \n",
    "        Let's start:\n",
    "\n",
    "        Minimum specification:\n",
    "        {data_dict['question']}\n",
    "\n",
    "        Context documents:\n",
    "        {formatted_texts}\n",
    "\n",
    "        Output:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine text and image message\n",
    "    messages += text_message\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "multimodal_rag = (\n",
    "        {\n",
    "            \"context\": itemgetter('context'),\n",
    "            \"question\": itemgetter('input'),\n",
    "        }\n",
    "            |\n",
    "        RunnableLambda(multimodal_prompt_function)\n",
    "            |\n",
    "        chatgpt\n",
    "            |\n",
    "        StrOutputParser()\n",
    ")\n",
    "\n",
    "# Pass input query to retriever and get context document elements\n",
    "retrieve_docs = (itemgetter('input')\n",
    "                    |\n",
    "                retriever_multi_vector\n",
    "                    |\n",
    "                RunnableLambda(split_image_text_types))\n",
    "\n",
    "# Below, we chain `.assign` calls. This takes a dict and successively\n",
    "# adds keys-- \"context\" and \"answer\"-- where the value for each key\n",
    "# is determined by a Runnable (function or chain executing at runtime).\n",
    "# This helps in also having the retrieved context along with the answer generated by GPT-4\n",
    "multimodal_rag_w_sources = (RunnablePassthrough.assign(context=retrieve_docs)\n",
    "                                               .assign(answer=multimodal_rag)\n",
    ")\n",
    "\n",
    "# this is the function that will be called for inference \n",
    "def multimodal_rag_qa(query):\n",
    "    response = multimodal_rag_w_sources.invoke({'input': query})\n",
    "    return response['answer'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delare the additional function the ensuing code will be using\n",
    "# assuming the first cell was not run as the documents were\n",
    "# loaded from vector store and document store which were persisted\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import pandas as pd\n",
    "import ollama\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for basic RAG to load, split and embed the documents\n",
    "\n",
    "def createstorefromdoc(path):\n",
    "    start_time = time.time()\n",
    "    loader = PyPDFDirectoryLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    persist_directory = 'basicrag_db'\n",
    "    vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                 embedding=embeddings,\n",
    "                                 persist_directory=persist_directory)\n",
    "    vectorstore.persist()\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"--- {elapsed} seconds ---\")\n",
    "    \n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 20.075923919677734 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DAVID\\AppData\\Local\\Temp\\ipykernel_77156\\1184484340.py:17: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.075923919677734"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the above function\n",
    "# It does not need to be run if the same vector store was persisted before\n",
    "createstorefromdoc('./datasheet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reload the vector store which was persisted earlier\n",
    "\n",
    "def loadvectorstore():\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    persist_directory = 'basicrag_db'\n",
    "    vectorstore = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function provide the inference for basic RAG\n",
    "# It accepts two parameters, vector store and the input\n",
    "# In this case, it will be a cell from excel file\n",
    "\n",
    "# This function concatenate the relevant chunks retrieved\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# This function implements basic RAG \n",
    "def basic_rag(vectorstore, question):   \n",
    "\n",
    "    # Create the retriever\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    formatted_prompt = f\"Minimum specification: {question}\\n\\nContext documents: {formatted_context}\"\n",
    "\n",
    "    text_message = f\"\"\"\n",
    "        You are an pre-sales specialist well-versed with the company products based on the datasheet and brochure available and\n",
    "        are competent in matching the requirements from potential user or clients with the available product specification. \n",
    "        You will be given Context documents which will be a mix of text, tables, and images usually of charts or graphs.\n",
    "        Use this information to match the specification for provided Minimum specification with the relevant specification offered in Context documents.\n",
    "        Your answer should specify a value or text extracted as a piece of information from Context document only. \n",
    "        DO NOT include preamble. \n",
    "        DO NOT give me the sources where you obtain your information or conclusion. \n",
    "        DO NOT ask further questions.\n",
    "        DO NOT make up answers, use the provided context documents below and answer the question to the best of your ability. \n",
    "        Be as brief, complete and precise in your output as possible. \n",
    "\n",
    "        \"\"\"\n",
    " \n",
    "    response = ollama.chat(model='llama3.1:8b', messages=[{'role': 'system', 'content': text_message},{'role': 'user', 'content': formatted_prompt}])\n",
    "    \n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the final section where the inference output from multimodal and basic RAG \n",
    "# are channeled into filling up the separate columns in excel file for comparison\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('Sample-TenderDoc.xlsx')\n",
    "\n",
    "# placeholders for the responses from multimodal and basic RAG\n",
    "mm_responses = []\n",
    "basic_responses = []\n",
    "vstore = loadvectorstore()\n",
    "\n",
    "# Iterate over the rows and generate responses for multimodal and basic RAG\n",
    "# each response is based upon the entry in 'Minimum Specification' column\n",
    "for index, row in df.iterrows():\n",
    "    min_spec = row['Minimum Specification']\n",
    "    expected_response = row['Expected Response']\n",
    "    query = f\"Based on the datasheet, what is the closest specification that could meet or exceed this technical requirement: {min_spec} ?\"\n",
    "\n",
    "    # Generate basic RAG response\n",
    "    basic_response = basic_rag(vstore, query)\n",
    "    basic_responses.append(basic_response)\n",
    "\n",
    "    # Generate multimodal RAG response\n",
    "    mm_response = multimodal_rag_qa(query)\n",
    "    mm_responses.append(mm_response)\n",
    "    \n",
    "# Add the new columns to the DataFrame\n",
    "df['Multimodal RAG'] = mm_responses\n",
    "df['Basic RAG'] = basic_responses\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "df.to_excel('ProcessedFile.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response for row 0: Intel \n",
      "\n",
      "Processor type\n",
      "Generated response for row 1: 2nd Generation Intel® Xeon Scalable processors up to 28 cores per processor\n",
      "Generated response for row 2: Up to two 2nd Generation Intel Xeon Scalable processors\n",
      "Generated response for row 3: Internal Controllers: PERC H330, H730P, H740P, HBA330\n",
      "Generated response for row 4: Up to 128TB\n",
      "Generated response for row 5: Up to 24 DDR4 DIMM slots\n",
      "Generated response for row 6: This is a detailed spec sheet for the PowerEdge R740 server from Dell EMC. Here are some of the key features and specifications:\n",
      "\n",
      "**Key Features**\n",
      "\n",
      "* Persistent Memory NVDIMM-N can increase database performance by 10x\n",
      "* Wipe all data from storage media quickly and securely with System Erase\n",
      "* Support for up to three 300W or six 150W GPUs\n",
      "\n",
      "**Specifications**\n",
      "\n",
      "* Max 128TB SATA HDD storage\n",
      "* Optional DVD-ROM, DVD+RW drive\n",
      "* Titanium 750W power supply (platinum 495W, 750W also available)\n",
      "* Hot plug power supplies with full redundancy (750W 240VDC, 1100W 380VDC2, 1600W, 2000W, and 2400W)\n",
      "* Up to 6 hot plug fans with full redundancy (1100W 380VDC2)\n",
      "* Gold 1100W -48VDC power supply\n",
      "* Form factor: Rack (2U)\n",
      "* Height: 86.8mm (3.4”)\n",
      "* Width: 434.0mm (17.08”)\n",
      "* Depth: 737.5mm (29.03”)\n",
      "* Weight: 28.6kg (63lbs.)\n",
      "\n",
      "**Management and Security**\n",
      "\n",
      "* Embedded management iDRAC9, iDRAC Direct, iDRAC RESTful with Redfish\n",
      "* System Lockdown (requires iDRAC Enterprise or Datacenter)\n",
      "* TPM 1.2/2.0, TCM 2.0 optional\n",
      "\n",
      "**Ports and I/O**\n",
      "\n",
      "* Network daughter card options: 4 x 1GbE or 2 x 10GbE + 2 x 1GbE or 4 x 10GbE or 2 x 25GbE\n",
      "* Front ports: 1 x Dedicated iDRAC Direct Micro-USB, 2 x USB 2.0, 1 x USB 3.0 (optional), 1 x VGA\n",
      "* Rear ports: 1 x Dedicated iDRAC network port, 1 x Serial, 2 x USB 3.0, 1 x VGA\n",
      "\n",
      "**Accelerators and Operating Systems**\n",
      "\n",
      "* Support for up to three double-width or four single-width FPGAs\n",
      "* Supported operating systems: Canonical Ubuntu Server LTS, Red Hat Enterprise Linux, Citrix Hypervisor, SUSE Linux Enterprise Server, Microsoft Windows Server LTSC with Hyper-V, VMware ESXi, Oracle Linux\n",
      "\n",
      "**OEM-Ready Version**\n",
      "\n",
      "* Available for custom branding and packaging\n",
      "\n",
      "Note that some features may vary by region or country.\n",
      "Generated response for row 7: Up to three 300W or six 150W GPUs\n",
      "Generated response for row 8: Canonical® Ubuntu® Server LTS\n",
      "Generated response for row 9: Dimensions: Height: 86.8mm (3.4”), Width: 434.0mm (17.08”), Depth: 737.5mm (29.03”)\n"
     ]
    }
   ],
   "source": [
    "# The following are the inference code for multimodal RAG only, just as reference\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('Sample-TenderDoc.xlsx')\n",
    "\n",
    "# Iterate over the rows and generate responses and evaluations\n",
    "generated_responses = []\n",
    "# evaluation_scores = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    min_spec = row['Minimum Specification']\n",
    "    expected_response = row['Expected Response']\n",
    "    query = f\"Based on the datasheet, what is the closest specification that could meet or exceed this technical requirement: {min_spec} ?\"\n",
    "\n",
    "    # Generate response\n",
    "    generated_response = multimodal_rag_qa(query)\n",
    "    generated_responses.append(generated_response)\n",
    "    \n",
    "# Add the new columns to the DataFrame\n",
    "df['Multimodal RAG'] = generated_responses\n",
    "# df['Evaluation Score'] = evaluation_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "df.to_excel('ProcessedFile.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_10_14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
